{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Scfr942GwO4"
      },
      "source": [
        "<img src=\"https://nnsight.net/_images/nnsight_logo.svg\" alt=\"drawing\" width=\"200\"/>\n",
        "\n",
        "# **NNsight**\n",
        "## The API for a transparent science on black-box AI\n",
        "\n",
        "In this era of large-scale deep learning, the most interesting AI models are massive black boxes that are hard to run. Ordinary commercial inference service APIs let you interact with huge models, but not model internals.\n",
        "\n",
        "The NNsight library is different: it gives you (yes you) full access to all the neural network internals. When used together with a remote service like the [National Deep Inference Facility](https://thevisible.net/docs/NDIF-proposal.pdf) (NDIF), it lets you run complex experiments on huge open source models easily, with fully transparent access.\n",
        "\n",
        "Our team wants to enable entire labs and independent researchers alike, as we believe a large, passionate, and collaborative community will produce the next big insights on a profoundly important field.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1OemD2VGyZx"
      },
      "source": [
        "# But first, let's start small\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyLMmhrAKTNM"
      },
      "source": [
        "## The Tracing Context\n",
        "\n",
        "To demonstrate the core funtionality and syntax of NNsight, we'll define and use a toy two layer neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R0RJijD0eXwf"
      },
      "outputs": [],
      "source": [
        "# # Install nnsight\n",
        "# !pip install git+https://github.com/JadenFiotto-Kaufman/nnsight.git@dev\n",
        "\n",
        "# from IPython.display import clear_output\n",
        "\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgydw7i3HmIH"
      },
      "source": [
        "Our little model here is composed of two two linear layers. We specify the sizes of each of these modules, and create some complementary example input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2pX2Wg8Ceo6N"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "from rich import print as rprint\n",
        "\n",
        "\n",
        "input_size = 5\n",
        "hidden_dims = 10\n",
        "output_size = 2\n",
        "\n",
        "net = torch.nn.Sequential(OrderedDict([\n",
        "    ('layer1', torch.nn.Linear(input_size, hidden_dims)),\n",
        "    ('layer2', torch.nn.Linear(hidden_dims, output_size)),\n",
        "])).requires_grad_(False)\n",
        "\n",
        "input = torch.rand((1, input_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIPa2h2pJIwl"
      },
      "source": [
        "The core object of the nnsight package is `NNsight`. This wraps around a given PyTorch model to enable the NNsight's capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8H9R_ynTJI5y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from nnsight import NNsight\n",
        "\n",
        "model = NNsight(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NfISlQ_Ilvp"
      },
      "source": [
        "When printed, PyTorch models show a named hierarchy of modules. This is a useful reference when for accessing sub-components directly, and NNsight models work just the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYtnbJHvlGZV",
        "outputId": "09d8fbb8-e7f1-4ceb-f15f-12a70fd36c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djC5kyJWLuUH"
      },
      "source": [
        "Before we actually get to using the model we just created, let's talk about what a `context` is in Python.\n",
        "\n",
        "Often enough, when coding, you want to create some object, or initiate some logic, that you later want to destroy or conclude.\n",
        "\n",
        "The most common application is opening files like the following example:\n",
        "\n",
        "```python\n",
        "with open('myfile.txt', 'r') as file:\n",
        "  text = file.read()\n",
        "```\n",
        "\n",
        "Python uses the `with` keyword to enter a context-like object. This object defines logic to be ran at the start of the with block, and logic to be ran when exiting. In this case, entering the context opens a file and exiting the context closes it. Being within the context means we can read from file. Simple enough! Now we can discuss how NNsight uses contexts to enable powerful and intuitive access into the internals of model computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNvuCOeyojcA"
      },
      "source": [
        "Introducing the tracing context. Just like before, something happens upon entering the tracing context, something happens when exiting, and being inside enables some functionality.\n",
        "\n",
        "We enter the tracing context by calling `.trace(<input>)` on the `NNsight` model we created before. Entering it denotes we want to run the model given our input... but not yet! The model is only ran upon exiting the tracing context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qEXQ4auPSL-m"
      },
      "outputs": [],
      "source": [
        "with model.trace(input) as tracer:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZQsHinjqicJ"
      },
      "source": [
        "But where's the output? To get that, we'll have to learn how to request it from within the tracing conext."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMfBpYzDPMoB"
      },
      "source": [
        "## Getting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_aFwFRv0ax"
      },
      "source": [
        "Earlier, when we wrapped our toy model with the `NNsight` class, this added a couple properties to each module in the model (including the root model itself). The ones we care about are `.input` and `.output`.\n",
        "\n",
        "```python\n",
        "model.input\n",
        "model.output\n",
        "```\n",
        "\n",
        "The names are pretty self explanatory. They correspond to the inputs and outputs of their respective modules during some forward pass of an input through the model. These are what we're going to interact with in the tracing context.\n",
        "\n",
        "However, the model isnt executed until the end of the tracing context, so we can't access the actual inputs and outputs of components within the context. For example, if we tried to print a module's output within a context, we'd just see an InterventionProxy.\n",
        "\n",
        "`.input` and `.output` are Proxies for the eventual inputs and outputs of a module. In other words, when you access `model.output`, you're telling `NNsight`: \n",
        "> \"When you compute the output of `model`, please grab it for me and put the value into its corresponding Proxy object's `.value` attribute.\"\n",
        "\n",
        "Let's try just that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "cYe8-r9ptGaG",
        "outputId": "f743c3b6-e4ab-4b8b-f964-fb7eef8860f2"
      },
      "outputs": [],
      "source": [
        "# with model.trace(input) as tracer:\n",
        "\n",
        "#   output = model.output\n",
        "\n",
        "# print(output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBz5qfwuR-6F"
      },
      "source": [
        "Oh no, an error! `\"Accessing Proxy value before it's been set.\"`\n",
        "\n",
        "If `.value` isn't filled in after leaving the tracing context, accessing the value will give you this error.  In reality however, the value was filled in, it was just immediately removed. Why?\n",
        "\n",
        "Proxy objects track their listeners (as in other Proxy object that rely on it), and when their listeners are all complete, it deletes the `.value` associated with the Proxy in order to save memory. To prevent this, we call `.save()` on the Proxy objects we want to access outisde of the tracing context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_bXRd5dvsBu",
        "outputId": "dee73e0c-1ca7-443f-9ddc-1f1e30f33a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.2326,  0.0592]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "  output = model.output.save()\n",
        "\n",
        "print(output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5C0UZCwvrYn"
      },
      "source": [
        "Success! We now have the model output meaning ou just completed your first intervention request using Proxies.\n",
        "\n",
        "These requests are handled at the soonest possible moment they can be completed. In this case, right after the model's output was computed. We call this process `interleaving`.\n",
        "\n",
        "What else can we request? There's nothing special about the model itself vs it's submodules. Just like we saved the output of the model as a whole, we can save the output of any of it's submodules. To get to them we use normal Python attribute syntax, and we know where the modules are becuase we printed out the model earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WcVUSP-0CJi",
        "outputId": "202836e4-1e94-4bee-870c-21a9a895c91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akWr-cNqy-9O",
        "outputId": "d873fb2a-6c42-4bab-964d-806163dd6f2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1175,  0.1461, -0.1080, -0.2595,  0.1440, -0.0546, -0.0097,  0.8252,\n",
            "          0.1391, -0.4482]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "  l1_output = model.layer1.output.save()\n",
        "\n",
        "print(l1_output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85A-aP_03ht6"
      },
      "source": [
        "Let's do the same for the input of layer2. While we're at it, let's also drop the `as tracer`, as we won't be needing the tracer object itself for a few sections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EHEN38N3nXR",
        "outputId": "34db5654-00b3-406a-d350-09362986b04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((tensor([[-0.1175,  0.1461, -0.1080, -0.2595,  0.1440, -0.0546, -0.0097,  0.8252,\n",
            "          0.1391, -0.4482]]),), {})\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  l2_input = model.layer2.input.save()\n",
        "\n",
        "print(l2_input.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk-U8zi33Gi-"
      },
      "source": [
        "<details>\n",
        "  <summary>On module inputs</summary>\n",
        "\n",
        "  ---\n",
        "\n",
        "  Notice how the value for l2_input, was not just a single tensor.\n",
        "  The type/shape of values from .input is in the form of:\n",
        "\n",
        "      tuple(tuple(args), dictionary(kwargs))\n",
        "\n",
        "  Where the first index of the tuple is itself a tuple of all positional arguments, and the second index is a dictionary of the keyword arguments.\n",
        "\n",
        "  ---\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "Now that we can access activations, we also want to do some post-processing on it. Let's find out which dimension of layer1's output has the highest value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Xo_PHyPr4p"
      },
      "source": [
        "## Functions, Methods, and Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehSofWbx5DSx"
      },
      "source": [
        "We could do this by calling `torch.argmax(...)` after the tracing context... or we can just leverage the fact that `nnsight` handles functions and methods within the tracing context, by creating a Proxy request for it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5XCiSZn2p3k",
        "outputId": "8d7ff220-0212-4448-a480-554843284406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Note we don't need to call .save() on the output,\n",
        "  # as we're only using it's value within the tracing context.\n",
        "  l1_output = model.layer1.output\n",
        "\n",
        "  l1_amax = torch.argmax(l1_output, dim=1).save()\n",
        "\n",
        "print(l1_amax[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGPUJvWq_bOp"
      },
      "source": [
        "Nice! That worked seamlessly, but hold on, how come we didn't need to call `.value[0]` on the result? In previous sections, we were just being explicit to get an understaing of Proxies and their value. In practice however, `nnsight` knows that when outside of the tracing context we only care about the actual value, and so printing, indexing, and applying functions all immediately return and reflect the data in `.value`. So for the rest of the tutorial we won't use it.\n",
        "\n",
        "The same principles work for methods and operations as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIcOYeEjFuln",
        "outputId": "5899aeae-5e3b-4bc4-a0b6-230612861d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0834)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  value = (model.layer1.output.sum() + model.layer2.output.sum()).save()\n",
        "\n",
        "print(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0vNOJtJ2oFR"
      },
      "source": [
        "Getting and analyzing the activations from various points in a model can be really insightful, and a number of ML techniques do exactly that. However, often times we not only want to view the computation of a model, but influence it as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_5qH5gHPOT_"
      },
      "source": [
        "## Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgju-b_IOLlq"
      },
      "source": [
        "To demonstrate editing the flow of infomration through the model, let's set the first dimension of the first layer's output to 0. `NNsight` makes this really easy using the familiar assignment `=` operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6y2wzJqOz3a",
        "outputId": "40452df1-51cd-4ec3-bfc5-6d069fd70601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1175,  0.1461, -0.1080, -0.2595,  0.1440, -0.0546, -0.0097,  0.8252,\n",
            "          0.1391, -0.4482]])\n",
            "tensor([[ 0.0000,  0.1461, -0.1080, -0.2595,  0.1440, -0.0546, -0.0097,  0.8252,\n",
            "          0.1391, -0.4482]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Save the output before the edit to compare.\n",
        "  # Notice we\n",
        "  l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "  # Access the 0th index of the hidden state dimension ans set it to 0.\n",
        "  model.layer1.output[:, 0] = 0\n",
        "\n",
        "  # Save the output after to see our edit.\n",
        "  l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(l1_output_before.value)\n",
        "print(l1_output_after.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZz9SMs3Y_iS"
      },
      "source": [
        "Note the use of `.clone()` to save the output of layer 1 before we applied an edit. Because `.save()` returns a reference to values *after* computation, we use `.clone()` to copy the value before edits are applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s016CelFP8sx"
      },
      "source": [
        "## Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome! Now we know how to access and intervene on a simple model's forward pass. What about the backward pass?\n",
        "\n",
        "Lets declare the same toy model, this time with `.requires_grad_(True)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = torch.nn.Sequential(OrderedDict([\n",
        "    ('layer1', torch.nn.Linear(input_size, hidden_dims)),\n",
        "    ('layer2', torch.nn.Linear(hidden_dims, output_size)),\n",
        "]))\n",
        "\n",
        "model = NNsight(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a simple example, let's do a backward pass on the sum of the logits. We can save gradients just as we do activations, calling `.save()` on the gradients `.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3012, -0.5661, -0.3582, -0.5639,  0.8994,  0.7014,  0.0738,  0.2688,\n",
            "         -0.2155,  0.1100]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.0437, -0.4045,  0.3466, -0.0019,  0.4575, -0.0447,  0.4492, -0.3388,\n",
            "          0.1754, -0.2691]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "    l1_output = model.layer1.output.save()\n",
        "    \n",
        "    l1_gradients = model.layer1.output.grad.save()\n",
        "\n",
        "    model.output.sum().backward()\n",
        "\n",
        "print(l1_output)\n",
        "print(l1_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note two things about the example above:\n",
        "1. Inference mode is off by default.\n",
        "2. We can call `.backward()` on a value within the tracing context just like you normally would.\n",
        "\n",
        "With that, we've learned the basics of getting and setting activations and gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvQ1nZgYQDG3"
      },
      "source": [
        "# Bigger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJDblHiQpp1"
      },
      "source": [
        "## LanguageModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*I thought you said one trillion parameter models!*\n",
        "\n",
        "Yes! NNsight provides the `LanguageModel` class which allows us to load models by their HuggingFace repo-id.\n",
        "\n",
        "<details>\n",
        "  <summary>On custom models</summary>\n",
        "\n",
        "  ---\n",
        "\n",
        "  You can also pass in custom models! Note that you'll have to pass in a tokenizer as well as the model. \n",
        "\n",
        "    model = LanguageModel(<CUSTOM_MODEL>, tokenizer=<TOKENIZER>)\n",
        "\n",
        "  ---\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnsight import LanguageModel\n",
        "\n",
        "model = LanguageModel('openai-community/gpt2', device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can access the models tokenizer with `.tokenizer`, and it will return the default HuggingFace tokenizer for that `AutoModel` class. \n",
        "\n",
        "A couple of useful methods for encoding and decoding are listed below. See the official HuggingFace page [here](https://huggingface.co/docs/transformers/main_classes/tokenizer) for more arguments and methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: [7554, 290, 5335, 1816, 284, 262, 3650, 13, 1757, 10158, 262, 7545, 284]\n",
            "Token Tensor: tensor([[ 7554,   290,  5335,  1816,   284,   262,  3650,    13,  1757, 10158,\n",
            "           262,  7545,   284]])\n",
            "String Tokens: ['John', 'Ġand', 'ĠMary', 'Ġwent', 'Ġto', 'Ġthe', 'Ġstore', '.', 'ĠJohn', 'Ġhanded', 'Ġthe', 'Ġmilk', 'Ġto']\n",
            "Decoded: John and Mary went to the store. John handed the milk to\n"
          ]
        }
      ],
      "source": [
        "tokenizer = model.tokenizer\n",
        "\n",
        "prompt = \"John and Mary went to the store. John handed the milk to\"\n",
        "\n",
        "tokens = tokenizer.encode(prompt) # str -> list[int]\n",
        "token_tensor = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"] # str -> Tensor[int]\n",
        "\n",
        "str_tokens = tokenizer.convert_ids_to_tokens(tokens) # list[int] -> list[str]\n",
        "decoded = tokenizer.decode(tokens) # list[int] -> str\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token Tensor:\", token_tensor)\n",
        "print(\"String Tokens:\", str_tokens)\n",
        "print(\"Decoded:\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not only can `LanguageModel` take a tensor as input, but it will automatically tokenize text or batches of text. Let's try passing in a batch of text, then decoding an output. \n",
        "\n",
        "Setting `trace=False` tells NNsight that we don't plan to access or intervene on any hidden states; this simplifies syntax quite a bit and directly returns the model's output.\n",
        "\n",
        "<details>\n",
        "  <summary>Passing arguments to tracer</summary>\n",
        "\n",
        "  ---\n",
        "\n",
        "  If you pass input directly to the `trace` context, you can include arguments for tokenization in `invoker_args`. They will be passed downstream to the invoker and then to _prepare_inputs. See the later section on Batching for more information about the invoker context.\n",
        "\n",
        "    model.trace(<TEXT>, invoker_args={\"key\",\"value\"}):\n",
        "\n",
        "  ---\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run our prompt through the model. Setting `trace=False` tells NNsight that we don't plan to access or intervene on any hidden states; this simplifies syntax quite a bit and directly returns the model's output.\n",
        "\n",
        "<details>\n",
        "  <summary>Passing arguments to tracer</summary>\n",
        "\n",
        "  ---\n",
        "\n",
        "  If you pass input directly to the `trace` context, you can include arguments for tokenization in `invoker_args`. They will be passed downstream to the invoker and then to _prepare_inputs. See the later section on Batching for more information about the invoker context.\n",
        "\n",
        "    model.trace(<TEXT>, invoker_args={\"key\",\"value\"}):\n",
        "\n",
        "  ---\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next token prediction:  Mary\n"
          ]
        }
      ],
      "source": [
        "output = model.trace(prompt, trace=False)\n",
        "tokens = output.logits.softmax(-1).argmax(-1)\n",
        "\n",
        "print(\"Next token prediction:\", tokenizer.decode(tokens[0,-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not only can `LanguageModel` take a tensor as input, but it will automatically tokenize text or batches of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 7554,   290,  5335,  1816,   284,   262,  3650,    13,  1757, 10158,\n",
            "           262,  7545,   284],\n",
            "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,  7554, 10158,\n",
            "           262,  7545,   284]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "batch_of_text = [prompt, \"John handed the milk to\"]\n",
        "\n",
        "with model.trace(batch_of_text):\n",
        "    prepared_input = model.input[1][\"input_ids\"].save()\n",
        "\n",
        "print(prepared_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note how the shorter prompt is padded. By default, LanguageModel will prepare text inputs with left sided padding. It will not prepend a `BOS` token.\n",
        "\n",
        "For easier indexing, you can use `.t[<idx>]` to index into tokens from the back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traced token: tensor([7554], device='cuda:0')\n",
            "Original token: tensor([7554])\n"
          ]
        }
      ],
      "source": [
        "toks = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "with model.trace(toks) as tracer:\n",
        "    token_0 = model.transformer.input[0][0].t[0].save()\n",
        "\n",
        "# Check that the value from t[0] is the same as the 0th token in the input.\n",
        "print(\"Traced token:\", token_0)\n",
        "print(\"Original token:\", toks[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this example forward, we'll be using a tokenized version of our text input to make it easier to refer to and compare shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting and Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything that we've learned so far works exactly the same with LanguageModel. LanguageModels tend to be quite a bit more complicated than the simple model we declared above. We can print the model to figure out how to correctly access its components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  (generator): WrapperModule()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To demonstrate, let's ablate over a token position at an arbitrary layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-34.9196, -34.4503, -37.5173,  ..., -42.6724, -41.8168, -34.7129]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(toks):\n",
        "    # Intervene on a module\n",
        "    model.transformer.h[-3].attn.output[0].t[0] = 0.\n",
        "\n",
        "    # Save the model's outputs\n",
        "    logits = model.output.logits.save()\n",
        "\n",
        "# Just print the first token's logits for visual clarity.\n",
        "print(logits[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How can we compare the difference in the clean (unedited) versus edited output? Let's try saving outputs with `.clone()` like we did earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-34.9196, -34.4503, -37.5173,  ..., -42.6724, -41.8168, -34.7129]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor([[-34.9196, -34.4503, -37.5173,  ..., -42.6724, -41.8168, -34.7129]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(toks):\n",
        "\n",
        "    output_one = model.output.logits.clone().save()\n",
        "\n",
        "    model.transformer.h[-3].attn.output[0].t[0] = 0.\n",
        "\n",
        "    output_two = model.output.logits.save()\n",
        "\n",
        "print(output_one[:,0])\n",
        "print(output_two[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wait, they're the same? Since the trace context operates on a single forward pass, you can't get the value of some later module and set it as the value of an earlier model.\n",
        "\n",
        "Instead, we can use batching and multi invoke contexts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember that `tracer` object that we dropped earlier? Well, we can call `.invoke()` on the tracer object to create an `invoke` context. Each `invoke` context takes an input, and those inputs are combined in a single forward pass through the model. However, all the operations we declare under each `invoke` context will only apply to the context's input.\n",
        "\n",
        "Let's see how this works in practice by solving our problem from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace() as tracer:\n",
        "    with tracer.invoke(toks):\n",
        "        output_one = model.lm_head.output.save()\n",
        "\n",
        "    with tracer.invoke(toks):\n",
        "        model.transformer.h[-3].attn.output[0][:] = 0.\n",
        "\n",
        "        output_two = model.lm_head.output.save()\n",
        "\n",
        "print(output_one[:,0])\n",
        "print(output_two[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how we're calling `model.lm_head.output` instead of `model.output` here. Since operations under each `invoke` context are acting on a slice of the larger `trace` batch, we can only operate on tensors or other collections. `model.output` is a `CausalLMOutput` object which can't be indexed.\n",
        "\n",
        "One cool thing we can do is pass values across invoke contexts. Let's say we wanted to use the output of the last layer as the input to the first layer. We can do that as such:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace() as tracer:\n",
        "    with tracer.invoke(toks):\n",
        "        final_layer_output = model.transformer.h[-1].output[0]\n",
        "\n",
        "    with tracer.invoke(toks):\n",
        "        model.transformer.h[0].attn.output[0][:] = final_layer_output\n",
        "\n",
        "        output = model.output.logits.save()\n",
        "\n",
        "print(output[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## .next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've talked a lot about accessing and intervening on a single forward pass. But the generative capabilities of modern language models is part of what makes them so impactful. NNsight enables users to access and intervene at every step of generation.\n",
        "\n",
        "Using the same LanguageModel as before, we can declare a new context, `.generate`. The keyword argument, `max_new_tokens` determines how many tokens we'll generate. To get the generated output, call `.model.generator.output.save()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.generate(toks, max_new_tokens=3) as tracer:\n",
        "    output = model.generator.output.save()\n",
        "\n",
        "print(\"Original Shape:\", toks.shape)\n",
        "print(\"Original + Generated Tokens:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, operations within the generate context will act on the original prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.generate(toks, max_new_tokens=3) as tracer:\n",
        "    print(model.transformer.h[0].output[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To access the hidden states of generated token sequences, use `.next()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_tokens = 3\n",
        "with model.generate(toks, max_new_tokens=new_tokens) as tracer:\n",
        "    hidden_states1 = model.transformer.h[-1].output[0].save()\n",
        "\n",
        "    tracer.next()\n",
        "    \n",
        "    hidden_states2 = model.transformer.h[-1].output[0].save()\n",
        "\n",
        "    tracer.next()\n",
        "    \n",
        "    hidden_states3 = model.transformer.h[-1].output[0].save()\n",
        "\n",
        "    out = model.generator.output.save()\n",
        "\n",
        "print(hidden_states1.shape)\n",
        "print(hidden_states2.shape)\n",
        "print(hidden_states3.shape)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6CCinkpQ5O3"
      },
      "source": [
        "## Logit Lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's put what we've learned so far into practice. As an example, let’s track the probability with which the model outputs “Mary” as we move through layers. One technique for this is the logit lens: we’ll treat the language model’s decoder applied to the intermediate activations as giving a crude approximation to this probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"John and Mary went to the store. John handed the milk to\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decoder(x):\n",
        "  # decoder consists of final layer norm + unembedding\n",
        "  return model.lm_head(model.transformer.ln_f(x))\n",
        "\n",
        "all_probs = []\n",
        "with model.trace(prompt) as tracer:\n",
        "  for layer in model.transformer.h:\n",
        "    logits = decoder(layer.output[0]) # apply decoder to hidden state\n",
        "    logits = logits[0,-1,:] # only over the final token\n",
        "    probs = logits.softmax(dim=-1).save() # apply softmax to get probabilities and save\n",
        "    all_probs.append(probs)\n",
        "    # all_probs[i].value now stores the probability distribution over tokens after layer i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "mary_token = model.tokenizer(\" Mary\").input_ids[0] # token id for Mary\n",
        "\n",
        "px.line(\n",
        "    [layer_probs.value[mary_token].item() for layer_probs in all_probs],\n",
        "    title=\"Probability of Mary after each layer, according to logit lens\",\n",
        "    labels={\"value\":\"Probability\", \"index\":\"Layer\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpkX-LwBQZHo"
      },
      "source": [
        "# I thought you said huge models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Ks1LUvQaER"
      },
      "source": [
        "## Remote execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3zRm-7VRRov"
      },
      "source": [
        "#Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAp7sPlzRMiA"
      },
      "source": [
        "# Advanced\n",
        "\n",
        "Link to IOI patching\n",
        "Link to function vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCCZAolsRhGk"
      },
      "source": [
        "## IOI Patching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bakFdBpRRsna"
      },
      "source": [
        "## Mamba Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ruv1OOoPC4d"
      },
      "source": [
        "#NOTES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTQawFrFT5qP"
      },
      "source": [
        "In Graph, check module_proxy.proxy_value for tracer.invoker. If None, return node.value anyway. either its done or error!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kzHcQmwmB0M"
      },
      "source": [
        "Pass in input and immediately trace like .invoke from before.\n",
        "No more tracer.output. Just access the output of the model as a proxy.\n",
        "\n",
        "describe attribute access to get modules and call .output and .input\n",
        "\n",
        "describe model is ran only at end of with block and proies are populated\n",
        "\n",
        "describe you need .save() or else value is destroyed before you could access and it throws an error.\n",
        "\n",
        "(note that .input is a tuple of (<args as tuple>, <kwargs as dict>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK7s1WOSlNBR",
        "outputId": "f9862dce-0b15-4416-8551-a6cd44009fdd"
      },
      "outputs": [],
      "source": [
        "with model.trace(input1):\n",
        "\n",
        "  l2_input = model.layer2.input.save()\n",
        "\n",
        "  output = model.output.save()\n",
        "\n",
        "print(l2_input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC5fBnR3p6bZ"
      },
      "source": [
        "Notice no .value. You could still call it if you want to.\n",
        "It's still a proxy, but when you do an operation on a completed proxy, it dosent create a new proxy it just returns the resultant value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY4a6m7fp6iM",
        "outputId": "f23023a9-937d-422d-b3d6-d56d33024b87"
      },
      "outputs": [],
      "source": [
        "with model.trace(input1):\n",
        "\n",
        "  output = model.output.save()\n",
        "\n",
        "  print(type(output))\n",
        "  print(output)\n",
        "\n",
        "output = output * 2\n",
        "\n",
        "print(type(output))\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC7xzC8LrQ4y"
      },
      "source": [
        "The Tracer object also allows attribute access for the model. THis is nice when you want to use NNsight inline to quickly trace a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctyINgpqrQ_N",
        "outputId": "a68d0b9d-b9d3-4148-f3fb-f4d06794d60a"
      },
      "outputs": [],
      "source": [
        "with NNsight(net).trace(input1) as tracer:\n",
        "\n",
        "  output = tracer.output.save()\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwLDy1AKq9cs"
      },
      "source": [
        "Can still do the full-featured multi-invoke version of trace like .forward before\n",
        "\n",
        "Can use proxy variable in between invokes\n",
        "It handles batching them for you and the shapes of values in them will be only of the size of the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaqRMbzcrGby",
        "outputId": "98e0988c-f7b9-4250-f79d-ef89f6593c7e"
      },
      "outputs": [],
      "source": [
        "with model.trace() as tracer:\n",
        "\n",
        "  with tracer.invoke(input2):\n",
        "\n",
        "    l2_input = model.layer2.input\n",
        "\n",
        "  with tracer.invoke(input1):\n",
        "\n",
        "    model.layer2.input = l2_input\n",
        "\n",
        "    output = model.layer2.output.save()\n",
        "\n",
        "print(output)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bkwq-_F0sNv"
      },
      "source": [
        "If you just want to run the model and get the output, do trace=False\n",
        "\n",
        "To tracing context, just output\n",
        "\n",
        "(note if you pass inout directly to .trace, you can pass a dict of invoker_args for input preprocessing options like from tracer.invoke(...))\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgQe-SZ00sUV",
        "outputId": "02f10575-bf7f-401d-9a49-a68eb811a132"
      },
      "outputs": [],
      "source": [
        "output = model.trace(input1, trace=False)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDcL3gIs1feB"
      },
      "source": [
        "The NNsight class is designed to be exended in order to enable tracing functionality for more complex models.\n",
        "\n",
        "LangugeModel is one such extention.\n",
        "\n",
        "Use hf repo_id to load and trace model\n",
        "\n",
        "device_map is an underlying HF transformer arg to auto assign parameters to gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5VWTgFP2mvH"
      },
      "outputs": [],
      "source": [
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0i8Y_Pf1dJQ",
        "outputId": "7270863a-ff05-432d-9927-f087d8630b04"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel('gpt2', device_map=\"auto\")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHiCfTUp16s6"
      },
      "source": [
        "Works just like the simple case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mnqaavg2AYK"
      },
      "outputs": [],
      "source": [
        "with model.trace(\"Hello World!\"):\n",
        "\n",
        "\n",
        "  logits = model.lm_head.output.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32eO1M22225v"
      },
      "source": [
        "One extention LanguageModel adds is to have the exact same tracing features as before but instead using the .generate method on HF transformer models\n",
        "\n",
        "You can call .next() on modules to move the \"pointer\" to which iterations of .output and .input refer to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "vMboLRKd3G61",
        "outputId": "15d85bf3-f09f-4f1c-9aff-69caaaef3860"
      },
      "outputs": [],
      "source": [
        "with model.generate(\"The Eiffel Tower is in the city of\", max_new_tokens=3) as tracer:\n",
        "\n",
        "  logits_1 = model.lm_head.output.save()\n",
        "\n",
        "  logits_2 = model.lm_head.next().output.save()\n",
        "\n",
        "  logits_3 = model.lm_head.next().output.save()\n",
        "\n",
        "print(logits1)\n",
        "print(logits2)\n",
        "print(logits3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Uz3vrG3wow"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVi-qjYR3w3H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
