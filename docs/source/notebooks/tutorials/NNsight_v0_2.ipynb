{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Scfr942GwO4"
      },
      "source": [
        "<img src=\"https://nnsight.net/_images/nnsight_logo.svg\" alt=\"drawing\" width=\"200\"/>\n",
        "\n",
        "# **NNsight**\n",
        "## The API for a transparent science on black-box AI\n",
        "\n",
        "In this era of large-scale deep learning, the most interesting AI models are massive black boxes that are hard to run. Ordinary commercial inference service APIs let you interact with huge models, but they do not let you access model internals.\n",
        "\n",
        "The nnsight library is different: it gives you (yes you) full access to all the neural network internals. When used together with a remote service like the [National Deep Inference Facility](https://thevisible.net/docs/NDIF-proposal.pdf) (NDIF), it lets you run complex experiments on huge open source models easily, with fully transparent access.\n",
        "\n",
        "Our team wants to enable entire labs and independent researchers alike, as we believe a large, passionate, and collaborative community will produce the next big insights on a profoundly important field.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1OemD2VGyZx"
      },
      "source": [
        "# But first, let's start small\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyLMmhrAKTNM"
      },
      "source": [
        "## The Tracing Context\n",
        "\n",
        "To demonstrate the core funtionality and syntax of nnsight, we'll define and use a tiny two layer neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R0RJijD0eXwf"
      },
      "outputs": [],
      "source": [
        "# # Install nnsight\n",
        "# !pip install git+https://github.com/JadenFiotto-Kaufman/nnsight.git@dev\n",
        "\n",
        "# from IPython.display import clear_output\n",
        "\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgydw7i3HmIH"
      },
      "source": [
        "Our little model here is composed of four sub-modules, two linear layers ('layer1', 'layer2') and two activation functions ('sigma1', 'sigma2'). We spcecify the sizes of each of these modules, and create some complementary example input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2pX2Wg8Ceo6N"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "input_size = 5\n",
        "hidden_dims = 10\n",
        "output_size = 2\n",
        "\n",
        "net = torch.nn.Sequential(OrderedDict([\n",
        "    ('layer1', torch.nn.Linear(input_size, hidden_dims)),\n",
        "    ('layer2', torch.nn.Linear(hidden_dims, output_size)),\n",
        "])).requires_grad_(False)\n",
        "\n",
        "input = torch.rand((1, input_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIPa2h2pJIwl"
      },
      "source": [
        "The core object of the nnsight package is `NNsight`. This wraps around a given pytorch model to enable the capabilites nnsight provides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8H9R_ynTJI5y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from nnsight import NNsight\n",
        "\n",
        "model = NNsight(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NfISlQ_Ilvp"
      },
      "source": [
        "Pytorch models when printed, show a named hierarchy of modules which is very useful when accessing sub-components directly. NNsight models work just the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYtnbJHvlGZV",
        "outputId": "09d8fbb8-e7f1-4ceb-f15f-12a70fd36c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djC5kyJWLuUH"
      },
      "source": [
        "Before we actually get to using the model we just created, let's talk about what a `context` is in Python.\n",
        "\n",
        "Often enough when coding, you want to create some object, or initiate some logic, that you later want to destroy or conclude.\n",
        "\n",
        "The most common application is opening files like the following example:\n",
        "\n",
        "```python\n",
        "with open('myfile.txt', 'r') as file:\n",
        "  text = file.read()\n",
        "```\n",
        "\n",
        "Python uses the `with` keyword to enter a context-like object. This object defines logic to be ran at the start of the with block, and logic to be ran when exiting. In this case, entering the context opens a file and exiting the context closes it. Being within the context means we can read from file. Simple enough! Now we can discuss how `nnsight` uses contexts to enable powerful and intuitive access into the internals of model computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNvuCOeyojcA"
      },
      "source": [
        "Introducing the tracing context. Just like before, something happens upon entering the tracing context, something happens when exiting, and being inside enables some functionality.\n",
        "\n",
        "We enter the tracing context by calling `.trace(<input>)` on the `NNsight` model we created before. Entering it denotes we want to run the model given our input... but not yet! The model is only ran upon exiting the tracing context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qEXQ4auPSL-m"
      },
      "outputs": [],
      "source": [
        "with model.trace(input) as tracer:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZQsHinjqicJ"
      },
      "source": [
        "But where's the output? To get that, we'll have to learn how to request it from within the tracing conext."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMfBpYzDPMoB"
      },
      "source": [
        "## Getting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_aFwFRv0ax"
      },
      "source": [
        "Earlier, when we wrapped out little net with the `NNsight` class, this added a couple properties on to each module in the model (including the root model itself). The ones we care about are `.input` and `.output`.\n",
        "\n",
        "```python\n",
        "model.input\n",
        "model.output\n",
        "```\n",
        "\n",
        "The names are pretty self explainatory. They correspond to the inputs and outputs of their respective modules during some forward pass of an input through the model. These are what we're going to interact with in the tracing context.\n",
        "\n",
        "However, remember how the model isnt executed until the end of the tracing context. So how can we access their inputs and outputs during computation from within the context? Well, we can't.\n",
        "\n",
        "`.input` and `.output` are Proxies for the eventual inputs and outputs of a module. In other words, when you access `model.output` what you're communicating to `nnsight` is \"When you compute the output of `model`, please grab it for me and put the value into it's corresponding Proxy object's `.value` attribute.\" Let's try just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "cYe8-r9ptGaG",
        "outputId": "f743c3b6-e4ab-4b8b-f964-fb7eef8860f2"
      },
      "outputs": [],
      "source": [
        "# with model.trace(input) as tracer:\n",
        "\n",
        "#   output = model.output\n",
        "\n",
        "# print(output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBz5qfwuR-6F"
      },
      "source": [
        "Oh no an error! \"Accessing Proxy value before it's been set.\"\n",
        "\n",
        "If `.value` isn't filled in after leaving the tracing context, accessing the value will give you this error.  In reality however, the value was filled in, it was just immediately removed. Why?\n",
        "\n",
        "Proxy objects track their listeners (as in other Proxy object that rely on it), and when their listeners are all complete, it deletes the `.value` associated with the Proxy in order to save memory. To prevent this, we call `.save()` on the Proxy objects we want to access outisde of the tracing context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_bXRd5dvsBu",
        "outputId": "dee73e0c-1ca7-443f-9ddc-1f1e30f33a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.3527, -0.3015]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "  output = model.output.save()\n",
        "\n",
        "print(output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5C0UZCwvrYn"
      },
      "source": [
        "Success! We now have the model output meaning ou just completed your first intervention request using Proxies.\n",
        "\n",
        "These requests are handled at the soonest possible moment they can be completed. In this case, right after the model's output was computed. We call this process `interleaving`.\n",
        "\n",
        "What else can we request? There's nothing special about the model itself vs it's submodules. Just like we saved the output of the model as a whole, we can save the output of any of it's submodules. To get to them we use normal Python attribute syntax, and we know where the modules are becuase we printed out the model earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WcVUSP-0CJi",
        "outputId": "202836e4-1e94-4bee-870c-21a9a895c91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akWr-cNqy-9O",
        "outputId": "d873fb2a-6c42-4bab-964d-806163dd6f2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2428,  0.8037, -0.5060, -0.5668, -0.0290,  0.8558, -0.5593, -0.1622,\n",
            "          0.1340,  0.1576]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "  l1_output = model.layer1.output.save()\n",
        "\n",
        "print(l1_output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85A-aP_03ht6"
      },
      "source": [
        "Let's do the same for the input of layer2. While we're at it, let's also drop the `as tracer`, as we won't be needing the tracer object itself for a few sections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EHEN38N3nXR",
        "outputId": "34db5654-00b3-406a-d350-09362986b04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((tensor([[ 0.2428,  0.8037, -0.5060, -0.5668, -0.0290,  0.8558, -0.5593, -0.1622,\n",
            "          0.1340,  0.1576]]),), {})\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  l2_input = model.layer2.input.save()\n",
        "\n",
        "print(l2_input.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk-U8zi33Gi-"
      },
      "source": [
        "<details>\n",
        "  <summary>On module inputs</summary>\n",
        "\n",
        "  ---\n",
        "\n",
        "  Notice how the value for l2_input, was not just a single tensor.\n",
        "  The type/shape of values from .input is in the form of:\n",
        "\n",
        "      tuple(tuple(args), dictionary(kwargs))\n",
        "\n",
        "  Where the first index of the tuple is itself a tuple of all positional arguments, and the second index is a dictionary of the keyword arguments.\n",
        "\n",
        "  ---\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "Now that we can access activations, we also want to do some post-processing on it. Let's find out which dimension of layer1's output has the highest value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Xo_PHyPr4p"
      },
      "source": [
        "## Functions, Methods, and Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehSofWbx5DSx"
      },
      "source": [
        "We could do this by calling `torch.argmax(...)` after the tracing context... or we can just leverage the fact that `nnsight` handles functions and methods within the tracing context, by creating a Proxy request for it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5XCiSZn2p3k",
        "outputId": "8d7ff220-0212-4448-a480-554843284406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(5)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Note we don't need to call .save() on the output,\n",
        "  # as we're only using it's value within the tracing context.\n",
        "  l1_output = model.layer1.output\n",
        "\n",
        "  l1_amax = torch.argmax(l1_output, dim=1).save()\n",
        "\n",
        "print(l1_amax[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGPUJvWq_bOp"
      },
      "source": [
        "Nice! That worked seamlessly, but hold on, how come we didn't need to call `.value[0]` on the result? In previous sections, we were just being explicit to get an understaing of Proxies and their value. In practice however, `nnsight` knows that when outside of the tracing context we only care about the actual value, and so printing, indexing, and applying functions all immediately return and reflect the data in `.value`. So for the rest of the tutorial we won't use it.\n",
        "\n",
        "The same principles work for methods and operations as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIcOYeEjFuln",
        "outputId": "5899aeae-5e3b-4bc4-a0b6-230612861d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-0.2837)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  value = (model.layer1.output.sum() + model.layer2.output.sum()).save()\n",
        "\n",
        "print(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0vNOJtJ2oFR"
      },
      "source": [
        "Getting and analyzing the activations from various points in a model can be really insightful, and a number of ML techniques do exactly that. However, often times we not only want to view the computation of a model, but influence it as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_5qH5gHPOT_"
      },
      "source": [
        "## Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgju-b_IOLlq"
      },
      "source": [
        "To demonstrate editing the flow of infomration through the model, let's set the first dimension of the first layer's output to 0. `NNsight` makes this really easy using the familiar assignment `=` operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6y2wzJqOz3a",
        "outputId": "40452df1-51cd-4ec3-bfc5-6d069fd70601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2428,  0.8037, -0.5060, -0.5668, -0.0290,  0.8558, -0.5593, -0.1622,\n",
            "          0.1340,  0.1576]])\n",
            "tensor([[ 0.0000,  0.8037, -0.5060, -0.5668, -0.0290,  0.8558, -0.5593, -0.1622,\n",
            "          0.1340,  0.1576]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Save the output before the edit to compare.\n",
        "  # Notice we\n",
        "  l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "  # Access the 0th index of the hidden state dimension ans set it to 0.\n",
        "  model.layer1.output[:, 0] = 0\n",
        "\n",
        "  # Save the output after to see our edit.\n",
        "  l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(l1_output_before.value)\n",
        "print(l1_output_after.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZz9SMs3Y_iS"
      },
      "source": [
        "Note the use of `.clone()` to save the output of layer 1 before we applied an edit. Because `.save()` returns a reference to values *after* computation, we use `.clone()` to copy the value before edits are applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s016CelFP8sx"
      },
      "source": [
        "## Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome! Now we know how to access and intervent on a simple model's forward pass. How about the backward pass?\n",
        "\n",
        "Lets declare the same toy model, this time with `.requires_grad_(True)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = torch.nn.Sequential(OrderedDict([\n",
        "    ('layer1', torch.nn.Linear(input_size, hidden_dims)),\n",
        "    ('layer2', torch.nn.Linear(hidden_dims, output_size)),\n",
        "]))\n",
        "\n",
        "model = NNsight(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a simple example, let's do a backward pass on the sum of the logits. We can save gradients just as we do activations, calling `.save()` on the gradients `.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1573,  0.8419, -0.1930,  0.2304, -0.2569,  0.1245,  0.2125, -0.4922,\n",
            "         -0.2421, -0.2142]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.3589,  0.2165,  0.5089, -0.5849,  0.0326,  0.1254,  0.0956, -0.2620,\n",
            "          0.1203,  0.2614]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "    l1_output = model.layer1.output.save()\n",
        "    \n",
        "    l1_gradients = model.layer1.output.grad.save()\n",
        "\n",
        "    model.output.sum().backward()\n",
        "\n",
        "print(l1_output)\n",
        "print(l1_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note two things about the example above:\n",
        "1. Inference mode is off by default.\n",
        "2. We can call `.backward()` on a value within the tracing context just like you normally would."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With that, we've learned the basics of getting and setting activations and gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvQ1nZgYQDG3"
      },
      "source": [
        "# Bigger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJDblHiQpp1"
      },
      "source": [
        "## LanguageModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*I thought you said one trillion parameter models!*\n",
        "\n",
        "Yes! NNsight provides the `LanguageModel` class which allows us to load models by their HuggingFace repo-id.\n",
        "\n",
        "<details>\n",
        "  <summary>On custom models</summary>\n",
        "\n",
        "  ---\n",
        "\n",
        "  You can also pass in custom models! Note that you'll have to pass in a tokenizer as well as the model. \n",
        "\n",
        "    model = LanguageModel(<CUSTOM_MODEL>, tokenizer=<TOKENIZER>, device_map=\"auto\")\n",
        "\n",
        "  ---\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnsight import LanguageModel\n",
        "\n",
        "model = LanguageModel('openai-community/gpt2', device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can access the models tokenizer with `.tokenizer`, and it will return the default HuggingFace tokenizer for that `AutoModel` class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6144, 18627, 318, 281, 7824, 329, 13245, 3783, 319, 2042, 3091, 9552, 13]\n",
            "['NN', 'sight', 'Ġis', 'Ġan', 'ĠAPI', 'Ġfor', 'Ġtransparent', 'Ġscience', 'Ġon', 'Ġblack', 'Ġbox', 'ĠAI', '.']\n",
            "NNsight is an API for transparent science on black box AI.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = model.tokenizer\n",
        "\n",
        "text = \"NNsight is an API for transparent science on black box AI.\"\n",
        "\n",
        "tokens = tokenizer.encode(text)\n",
        "str_tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
        "decoded = tokenizer.decode(tokens)\n",
        "\n",
        "print(tokens)\n",
        "print(str_tokens)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the `LanguageModel` class will tokenize text inputs. Note that it does not prepend a `BOS` token. Let's test it out, and decode logits into tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".. a open for thely. the holes..\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with model.trace(text):\n",
        "    logits = model.output.logits\n",
        "    tokens = logits.softmax(-1).argmax(-1).save()\n",
        "\n",
        "decoded = tokenizer.decode(tokens[0])\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything that we've done so far works exactly the same with LanguageModel - and across any NNsight model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  (generator): WrapperModule()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "input = \"NNsight is an API for transparent science on black box AI.\"\n",
        "\n",
        "with model.trace(input):\n",
        "    \n",
        "    # Save attn output at layer 3 to 0\n",
        "    l2_out = model.transformer.h[2].attn.output[0].save()\n",
        "\n",
        "    # Set the input to l0 as l2_out \n",
        "    model.transformer.h[0].attn.input[0][0][:] = l2_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note how we saved the output of a later layer, and set it as the value of an earlier layer's input. You can think of the operations within a context block as an unordered of instructions that execute on a model's forward pass. \n",
        "\n",
        "There are limits to this, however. Let's say we want to compare effect of an intervention on the model's logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace(\"test\"):\n",
        "    out_one = model.lm_head.output.clone().save()\n",
        "    model.transformer.h[0].attn.output[0][:] += 0.5\n",
        "    out_two = model.lm_head.output.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even if we called `.clone()` on `model.output`, the intervention would be applied anyways because the unembed, `lm_head`, occurs after the layers.\n",
        "\n",
        "We can use multiple invokes instead!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember that `tracer` object that we dropped earlier? Well, we can call `.invoke()` on the `tracer` object within a context, and run multiple forward passes. \n",
        "\n",
        "This solves the problem from before! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-12.5967, -11.0741, -14.2588,  ..., -19.6397, -19.2086, -11.7618],\n",
            "         [-16.2394, -11.5920, -14.8595,  ..., -24.8357, -23.7196, -16.7910],\n",
            "         [-38.9201, -34.3693, -40.0272,  ..., -46.5929, -47.3829, -37.1189]]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "tensor([[[-31.6922, -30.7873, -34.1027,  ..., -39.5088, -39.5361, -32.2880],\n",
            "         [-69.0204, -68.2261, -68.7214,  ..., -80.1794, -77.7888, -71.0367],\n",
            "         [-84.8164, -82.7565, -86.1548,  ..., -95.8974, -94.3171, -85.1944]]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "with model.trace() as tracer:\n",
        "    with tracer.invoke(\"input\") as invoker:\n",
        "        out_one = model.lm_head.output.save()\n",
        "\n",
        "    with tracer.invoke(\"input_tw\") as invoker:\n",
        "        model.transformer.h[0].attn.output[0][:] += 0.5\n",
        "        out_two = model.lm_head.output.save()\n",
        "\n",
        "print(out_one)\n",
        "print(out_two)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ability to call multiple invokes is actually pretty powerful. Tracer handles all operations as proxies, so we can freely pass values between `invoke` contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[12510, 1573, 9546, 13]\n",
            "[7571, 2456, 13]\n"
          ]
        }
      ],
      "source": [
        "input_one = \"Three word phrase.\"\n",
        "input_two = \"Two words.\"\n",
        "\n",
        "print(tokenizer.encode(input_one))\n",
        "print(tokenizer.encode(input_two))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "tensor([[-115.6758, -115.0759, -115.5405,  ..., -123.3979, -123.5300,\n",
            "         -109.5017],\n",
            "        [  11.8462,   13.1548,    8.6907,  ...,   -1.0914,    0.4976,\n",
            "           12.3454]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "with model.trace() as tracer:\n",
        "\n",
        "  with tracer.invoke(input_one):\n",
        "\n",
        "    l2_output = model.transformer.h[2].attn.output[0]\n",
        "    \n",
        "    print(l2_output.shape)\n",
        "\n",
        "  with tracer.invoke(input_two):\n",
        "\n",
        "    model.transformer.h[2].attn.output[0][:] = l2_output\n",
        "    \n",
        "    print(model.transformer.h[2].attn.output[0].shape)\n",
        "\n",
        "    output = model.output.save()\n",
        "\n",
        "print(output.logits[:,-1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvWA-CWqQtah"
      },
      "source": [
        "## .next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6CCinkpQ5O3"
      },
      "source": [
        "## Logit Lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpkX-LwBQZHo"
      },
      "source": [
        "# I thought you said huge models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Ks1LUvQaER"
      },
      "source": [
        "## Remote execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3zRm-7VRRov"
      },
      "source": [
        "#Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAp7sPlzRMiA"
      },
      "source": [
        "# Advanced\n",
        "\n",
        "Link to IOI patching\n",
        "Link to function vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCCZAolsRhGk"
      },
      "source": [
        "## IOI Patching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bakFdBpRRsna"
      },
      "source": [
        "## Mamba Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ruv1OOoPC4d"
      },
      "source": [
        "#NOTES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTQawFrFT5qP"
      },
      "source": [
        "In Graph, check module_proxy.proxy_value for tracer.invoker. If None, return node.value anyway. either its done or error!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kzHcQmwmB0M"
      },
      "source": [
        "Pass in input and immediately trace like .invoke from before.\n",
        "No more tracer.output. Just access the output of the model as a proxy.\n",
        "\n",
        "describe attribute access to get modules and call .output and .input\n",
        "\n",
        "describe model is ran only at end of with block and proies are populated\n",
        "\n",
        "describe you need .save() or else value is destroyed before you could access and it throws an error.\n",
        "\n",
        "(note that .input is a tuple of (<args as tuple>, <kwargs as dict>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK7s1WOSlNBR",
        "outputId": "f9862dce-0b15-4416-8551-a6cd44009fdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((tensor([[0.4196, 0.4998, 0.6265, 0.3975, 0.6541, 0.6097, 0.4248, 0.3506, 0.6187,\n",
            "         0.4576]], grad_fn=<SigmoidBackward0>),), {})\n",
            "tensor([[0.6444, 0.4773]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input1):\n",
        "\n",
        "  l2_input = model.layer2.input.save()\n",
        "\n",
        "  output = model.output.save()\n",
        "\n",
        "print(l2_input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC5fBnR3p6bZ"
      },
      "source": [
        "Notice no .value. You could still call it if you want to.\n",
        "It's still a proxy, but when you do an operation on a completed proxy, it dosent create a new proxy it just returns the resultant value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY4a6m7fp6iM",
        "outputId": "f23023a9-937d-422d-b3d6-d56d33024b87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'nnsight.intervention.InterventionProxy'>\n",
            "InterventionProxy (argument_1): FakeTensor(..., size=(1, 2), grad_fn=<SigmoidBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[1.2888, 0.9547]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input1):\n",
        "\n",
        "  output = model.output.save()\n",
        "\n",
        "  print(type(output))\n",
        "  print(output)\n",
        "\n",
        "output = output * 2\n",
        "\n",
        "print(type(output))\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC7xzC8LrQ4y"
      },
      "source": [
        "The Tracer object also allows attribute access for the model. THis is nice when you want to use NNsight inline to quickly trace a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctyINgpqrQ_N",
        "outputId": "a68d0b9d-b9d3-4148-f3fb-f4d06794d60a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.6444, 0.4773]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "with NNsight(net).trace(input1) as tracer:\n",
        "\n",
        "  output = tracer.output.save()\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwLDy1AKq9cs"
      },
      "source": [
        "Can still do the full-featured multi-invoke version of trace like .forward before\n",
        "\n",
        "Can use proxy variable in between invokes\n",
        "It handles batching them for you and the shapes of values in them will be only of the size of the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaqRMbzcrGby",
        "outputId": "98e0988c-f7b9-4250-f79d-ef89f6593c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.5843, -0.0754]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "with model.trace() as tracer:\n",
        "\n",
        "  with tracer.invoke(input2):\n",
        "\n",
        "    l2_input = model.layer2.input\n",
        "\n",
        "  with tracer.invoke(input1):\n",
        "\n",
        "    model.layer2.input = l2_input\n",
        "\n",
        "    output = model.layer2.output.save()\n",
        "\n",
        "print(output)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bkwq-_F0sNv"
      },
      "source": [
        "If you just want to run the model and get the output, do trace=False\n",
        "\n",
        "To tracing context, just output\n",
        "\n",
        "(note if you pass inout directly to .trace, you can pass a dict of invoker_args for input preprocessing options like from tracer.invoke(...))\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgQe-SZ00sUV",
        "outputId": "02f10575-bf7f-401d-9a49-a68eb811a132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.6444, 0.4773]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = model.trace(input1, trace=False)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDcL3gIs1feB"
      },
      "source": [
        "The NNsight class is designed to be exended in order to enable tracing functionality for more complex models.\n",
        "\n",
        "LangugeModel is one such extention.\n",
        "\n",
        "Use hf repo_id to load and trace model\n",
        "\n",
        "device_map is an underlying HF transformer arg to auto assign parameters to gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5VWTgFP2mvH"
      },
      "outputs": [],
      "source": [
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0i8Y_Pf1dJQ",
        "outputId": "7270863a-ff05-432d-9927-f087d8630b04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LanguageModel('gpt2', device_map=\"auto\")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHiCfTUp16s6"
      },
      "source": [
        "Works just like the simple case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mnqaavg2AYK"
      },
      "outputs": [],
      "source": [
        "with model.trace(\"Hello World!\"):\n",
        "\n",
        "\n",
        "  logits = model.lm_head.output.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32eO1M22225v"
      },
      "source": [
        "One extention LanguageModel adds is to have the exact same tracing features as before but instead using the .generate method on HF transformer models\n",
        "\n",
        "You can call .next() on modules to move the \"pointer\" to which iterations of .output and .input refer to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "vMboLRKd3G61",
        "outputId": "15d85bf3-f09f-4f1c-9aff-69caaaef3860"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-24c2e7e122b0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The Eiffel Tower is in the city of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mlogits_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mlogits_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nnsight/contexts/Runner.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m\"\"\"On exit, run and generate using the model whether locally or on the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-24c2e7e122b0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mlogits_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mlogits_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mlogits_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nnsight/module.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             self._output = self.tracer.graph.add(\n\u001b[1;32m    157\u001b[0m                 value=(\n\u001b[0;32m--> 158\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "with model.generate(\"The Eiffel Tower is in the city of\", max_new_tokens=3) as tracer:\n",
        "\n",
        "  logits_1 = model.lm_head.output.save()\n",
        "\n",
        "  logits_2 = model.lm_head.next().output.save()\n",
        "\n",
        "  logits_3 = model.lm_head.next().output.save()\n",
        "\n",
        "print(logits1)\n",
        "print(logits2)\n",
        "print(logits3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Uz3vrG3wow"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVi-qjYR3w3H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
